<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>BOOTPLACE: Bootstrapped Object Placement with Detection Transformers</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">BOOTPLACE: Bootstrapped Object Placement with Detection Transformers</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://ryanhangzhou.github.io/" target="_blank">Hang Zhou</a><sup>1</sup>&emsp;</span>
                <span class="author-block">
                  <a href="https://sites.google.com/site/xinxinzuohome/" target="_blank">Xinxin Zuo</a><sup>2</sup>&emsp;</span>
                  <span class="author-block">
                    <a href="https://ruim-jlu.github.io/#about" target="_blank">Rui Ma</a><sup>3</sup>&emsp;</span>
                    <span class="author-block">
                    <a href="https://vision-and-learning-lab-ualberta.github.io/author/li-cheng/" target="_blank">Li Cheng</a><sup>1</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">University of Alberta&emsp; Concordia University&emsp; Jilin University<br>CVPR25</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">

                  <!-- ArXiv abstract Link -->
                  <span class="link-block">
                    <a href="https://arxiv.org/abs/2503.21991" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/RyanHangZhou/BOOTPLACE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- Github link -->
                  <span class="link-block">
                    <a href="https://drive.google.com/file/d/1wOzpMPy3Vy0tdBRD0xC1eW3SO2aVeCVX/view?usp=sharing" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Dataset</span>
                  </a>
                </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero hero is-small">
  <div style="margin-top: -40px; display: flex; flex-direction: column; align-items: center; justify-content: center; text-align: center; padding: 0 25%;">
    <div class="text-center">
      <img src="static/images/teaser.png" width="100%" class="center">
    </div>
    <div style="margin-top: 5px; text-align: left;">
      <p>
        Our approach, BOOTPLACE, detects regions of interest (represented as bounding boxes) for object composition and assigns each target object to its best-matched detected region. Each object is connected to each detected region with weighted connections, with the bold arrow indicating the strongest link. 
      </p>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            In this paper, we tackle the copy-paste image-to-image composition problem with a focus on object placement learning. Prior methods have leveraged generative models to reduce the reliance for dense supervision. However, this often limits their capacity to model complex data distributions. Alternatively, transformer networks with a sparse contrastive loss have been explored, but their over-relaxed regularization often leads to imprecise object placement. We introduce BOOTPLACE, a novel paradigm that formulates object placement as a placement-by-detection problem. Our approach begins by identifying suitable regions of interest for object placement. This is achieved by training a specialized detection transformer on object-subtracted backgrounds, enhanced with multi-object supervisions. It then semantically associates each target compositing object with detected regions based on their complementary characteristics. Through a boostrapped training approach applied to randomly object-subtracted images, our model enforces meaningful placements through extensive paired data augmentation. Experimental results on established benchmarks demonstrate BOOTPLACE's superior performance in object repositioning, markedly surpassing state-of-the-art baselines on Cityscapes and OPA datasets with notable improvements in IOU scores. Additional ablation studies further showcase the compositionality and generalizability of our approach, supported by user study evaluations. 
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Paper method -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
        <div class="text-center">
          <img src="static/images/inference.png" width="100%" class="center">
        </div>
        <div class="content has-text-justified">
          <p>
            <b>Network inference.</b> Given a target image, several object queries (e.g., two cars and a pedestrian) and scene object locations, BOOTPLACE detects a set of candidate region of interest and associates each object with the best-fitting region, which are used to produce the composite image. &#9938; is feature concatenation and &#9663; is region-wise product. 
          </p>
          <div class="text-center">
          <img src="static/images/network.png" width="100%" class="center">
        </div>
        <div class="content has-text-justified">
          <p>
            <b>Network architecture and training.</b> We prepare training data by first decomposing a source image into a randomly-object-subtracted image <span style="font-family: cursive;">I</span> and a set of object queries. During training, image <span style="font-family: cursive;">I</span> and scene object locations are both fed into a detection transformer for region-of-interest detection. The object queries are fed into an association network for object-to-region matching, where the generated association links each object query with the detected region of interest. Losses comprises of detection loss and association loss. At the high level, we visualize the relations among object queries, detected regions of interest and ground-truth locations on the right side, where the best-matched association arrow is highlighted in bold. 
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper method -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/recomposition.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          <b>Qualitative results of object reposition</b> on Cityscapes dataset. Zoom in to see visual details. 1st column: original images with highlighted compositing objects; 2nd column: inpainted images after object subtraction. 
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/recomposition_OPA.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          <b>Qualitative results of object reposition</b> on OPA dataset. "Positive composites" are annotated good-quality composites from OPA. SAC-GAN is excluded as it requires semantic maps for training.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/composition.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
         <b>Qualitative results of object placement</b> on Cityscapes dataset. Objects are randomly chosen from Cityscapes testing set. 
       </h2>
     </div>
     <div class="item">
      <!-- Your image here -->
      <img src="static/images/generalizability.png" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        <b>Qualitative results of object placement</b> on Mapillary Vistas dataset. Objects are randomly chosen from Cityscapes testing set.
      </h2>
    </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->




<!-- Paper poster -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/poster.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{zhou2025bootplace,
            title={BOOTPLACE: Bootstrapped Object Placement with Detection Transformers},
            author={Zhou, Hang and Zuo, Xinxin and Ma, Rui and Cheng, Li},
            booktitle={Proceedings of the Computer Vision and Pattern Recognition Conference},
            pages={19294--19303},
            year={2025}
          }
</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<script type="text/javascript">
  var sc_project=12345678; 
  var sc_invisible=1; 
  var sc_security="abcdef12"; 
  var sc_https=1; 
  var sc_impression="1";
  var sc_url="https://www.statcounter.com";
</script>
<script type="text/javascript" src="https://www.statcounter.com/counter/counter.js"></script>

    <!-- End of Statcounter Code -->

  </body>
  </html>
